{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "y_Dn9rzyiqxi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from configs import get_config\n",
    "\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGLKx-6qiz-_"
   },
   "source": [
    "Upload the labels.csv and processed_counts.csv files to colab or your local workspace.\n",
    "\n",
    "This data associates a cell barcode, such as \"AAAGCCTGGCTAAC-1\", to a certain cell type label, such as \"CD14+ Monocyte\". For each cell barcode, there are also log RNA seq counts of 765 different genes, such as HES4.\n",
    "\n",
    "label.csv stores the association between a cell barcode and a cell type label.\n",
    "\n",
    "processed_counts.csv stores the normalized log read counts for each cell, where each row represents a single cell, and each column represents a gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WelsjSzviy4m"
   },
   "outputs": [],
   "source": [
    "labels_pd = pd.read_csv(\"labels.csv\")\n",
    "counts_pd = pd.read_csv(\"processed_counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((700, 2), (700, 766))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pd.shape, counts_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AAAGCCTGGCTAAC-1', 'AAATTCGATGCACA-1', 'AACACGTGGTCTTT-1'],\n",
       "      dtype='<U16')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = labels_pd['index'].to_numpy().astype(str)\n",
    "labels[[0,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'CD14+ Monocyte': 0,\n",
       "  'Dendritic': 1,\n",
       "  'CD56+ NK': 2,\n",
       "  'CD4+/CD25 T Reg': 3,\n",
       "  'CD19+ B': 4,\n",
       "  'CD8+ Cytotoxic T': 5,\n",
       "  'CD4+/CD45RO+ Memory': 6,\n",
       "  'CD8+/CD45RA+ Naive Cytotoxic': 7,\n",
       "  'CD4+/CD45RA+/CD25- Naive T': 8,\n",
       "  'CD34+': 9},\n",
       " {0: 'CD14+ Monocyte',\n",
       "  1: 'Dendritic',\n",
       "  2: 'CD56+ NK',\n",
       "  3: 'CD4+/CD25 T Reg',\n",
       "  4: 'CD19+ B',\n",
       "  5: 'CD8+ Cytotoxic T',\n",
       "  6: 'CD4+/CD45RO+ Memory',\n",
       "  7: 'CD8+/CD45RA+ Naive Cytotoxic',\n",
       "  8: 'CD4+/CD45RA+/CD25- Naive T',\n",
       "  9: 'CD34+'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2cell = {i:label for i, label in enumerate(labels_pd['bulk_labels'].unique())}\n",
    "cell2id = {label:i for i, label in id2cell.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 'CD14+ Monocyte',\n",
       "  1: 'Dendritic',\n",
       "  2: 'CD56+ NK',\n",
       "  3: 'CD4+/CD25 T Reg',\n",
       "  4: 'CD19+ B',\n",
       "  5: 'CD8+ Cytotoxic T',\n",
       "  6: 'CD4+/CD45RO+ Memory',\n",
       "  7: 'CD8+/CD45RA+ Naive Cytotoxic',\n",
       "  8: 'CD4+/CD45RA+/CD25- Naive T',\n",
       "  9: 'CD34+'},\n",
       " {'CD14+ Monocyte': 0,\n",
       "  'Dendritic': 1,\n",
       "  'CD56+ NK': 2,\n",
       "  'CD4+/CD25 T Reg': 3,\n",
       "  'CD19+ B': 4,\n",
       "  'CD8+ Cytotoxic T': 5,\n",
       "  'CD4+/CD45RO+ Memory': 6,\n",
       "  'CD8+/CD45RA+ Naive Cytotoxic': 7,\n",
       "  'CD4+/CD45RA+/CD25- Naive T': 8,\n",
       "  'CD34+': 9})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['CD14+ Monocyte','Dendritic','CD56+ NK','CD4+/CD25 T Reg','CD19+ B','CD8+ Cytotoxic T','CD4+/CD45RO+ Memory','CD8+/CD45RA+ Naive Cytotoxic','CD4+/CD45RA+/CD25- Naive T','CD34+']\n",
    "id2cell = {i:label for i, label in enumerate(labels)}\n",
    "cell2id = {label:i for i, label in id2cell.items()}\n",
    "id2cell, cell2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AAAGCCTGGCTAAC-1', 'AAATTCGATGCACA-1', 'AACACGTGGTCTTT-1',\n",
       "       'AAGTGCACGTGCTA-1', 'ACACGAACGGAGTG-1', 'ACAGTTCTTAGCCA-1',\n",
       "       'ACATTCTGACTACG-1', 'ACCCTCGAGTGAGG-1', 'ACTGGCCTTTTCGT-1',\n",
       "       'ACTTGGGAACCAGT-1', 'AGAAAGTGTGAACC-1', 'AGATATTGACCACA-1',\n",
       "       'AGTAAGGATTTACC-1', 'AGTTAAACAAACAG-1', 'ATAACAACCTCTAT-1',\n",
       "       'ATGGACACAAGTGA-1', 'ATGGACACTCGTTT-1', 'ATGGGTACCTGGTA-1',\n",
       "       'ATGTAAACTTTCGT-1', 'ATTAACGATACGAC-1'], dtype='<U16')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pd['index'].to_numpy().astype(str)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([700, 765])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_counts = counts_pd.iloc[:,1:].to_numpy().astype(np.float32)\n",
    "torch.from_numpy(numpy_counts).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUxSCyz7jBQf"
   },
   "source": [
    "Shuffle your data. Make sure your labels and the counts are shuffled together.\n",
    "\n",
    "Split into train and test sets (80:20 split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "XDTqBhcA7V8t"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I might want to separate out the labels/indicies from the counts into two separate datasets, stacking them at the end using torch.utils.data.StackDataset\n",
    "based on if it is training or not. I don't know if returning the labels and \n",
    "\"\"\"\n",
    "\n",
    "class CellDataset(Dataset):\n",
    "    def __init__(self, counts_csv_file:str, labels_csv_file:str, mode='train', test_size=0.2) -> tuple[torch.tensor, torch.tensor, np.ndarray]:\n",
    "        assert mode in ['train', 'test'], f'mode needs to be either train or test, but it\\'s {mode}'\n",
    "\n",
    "        self.labels = ['CD14+ Monocyte','Dendritic','CD56+ NK','CD4+/CD25 T Reg','CD19+ B','CD8+ Cytotoxic T','CD4+/CD45RO+ Memory','CD8+/CD45RA+ Naive Cytotoxic','CD4+/CD45RA+/CD25- Naive T','CD34+']\n",
    "        self.id2cell = {i:label for i, label in enumerate(self.labels)}\n",
    "        self.cell2id = {label:i for i, label in self.id2cell.items()}\n",
    "        \n",
    "        counts_pd = pd.read_csv(counts_csv_file)\n",
    "        numpy_counts = counts_pd.iloc[:,1:].to_numpy().astype(np.float32) # iloc[:, 1:] gets rid of the index column, leaving only normalized log count data\n",
    "\n",
    "        labels_pd = pd.read_csv(labels_csv_file)\n",
    "        id_labels = labels_pd['bulk_labels'].map(cell2id).to_numpy().astype(np.uint8)\n",
    "\n",
    "        partition = int(len(labels_pd) * (1 - test_size))\n",
    "    \n",
    "        if mode == 'train':\n",
    "            self.counts = torch.from_numpy(numpy_counts[:partition, :])\n",
    "            self.labels = torch.from_numpy(id_labels[:partition])\n",
    "            self.indicies = labels_pd['index'].to_numpy().astype(str)[:partition]\n",
    "        else:\n",
    "            self.counts = torch.from_numpy(numpy_counts[partition:, :])\n",
    "            self.labels = torch.from_numpy(id_labels[partition:])\n",
    "            self.indicies = labels_pd['index'].to_numpy().astype(str)[partition:]\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        log_counts = self.counts[idx]\n",
    "        label = self.labels[idx]\n",
    "        index = self.indicies[idx]\n",
    "        return log_counts, label, index\n",
    "\n",
    "    # def __getitems__(self, idxs):\n",
    "    #     counts = self.counts[idxs]\n",
    "    #     labels = self.labels[idxs]\n",
    "    #     indicies = self.indicies[idxs]\n",
    "    #     return counts, labels, indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([560]), torch.Size([140]))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = CellDataset(\"processed_counts.csv\", \"labels.csv\", mode='train')\n",
    "test_data = CellDataset(\"processed_counts.csv\", \"labels.csv\", mode='test')\n",
    "train_data.labels.shape, test_data.labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHIg7i1k7U-G"
   },
   "source": [
    "Create a fully connected neural network for your autoencoder. Your latent space can be of any size less than or equal to 64. Too large may result in a poor visualization, and too small may result in high loss. 32 is a good starting point.\n",
    "\n",
    "Consider using more than 1 hidden layer, and a sparcity constraint (l1 regularization).\n",
    "\n",
    "Have an encoder model which is a model of only the layers for the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = get_config(\"autoencoder_model.config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jump: 244.66666666666666\n"
     ]
    }
   ],
   "source": [
    "input_size, num_hidden_layers, output_size = 32, 3, 766\n",
    "jump = int(np.abs(np.ceil((input_size - output_size)/(num_hidden_layers))))\n",
    "print(f\"Jump: {np.abs((input_size - output_size) / (num_hidden_layers))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[766, 766]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = input_size\n",
    "layer_sizes = [input_size]\n",
    "for _ in range(num_hidden_layers - 1):\n",
    "    if input_size > output_size:\n",
    "        size -= jump\n",
    "    elif input_size < output_size:\n",
    "        size += jump\n",
    "    layer_sizes.append(size)\n",
    "else:\n",
    "    layer_sizes = [output_size for _ in range(num_hidden_layers - 1)]\n",
    "\n",
    "# layer_output_sizes.append(output_size)\n",
    "layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[32, 276], [276, 520]]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[layer_sizes[i: i + 2] for i in range(len(layer_sizes) - 2 + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "b8mvigLP7Sej"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'[' was never closed (683309363.py, line 10)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[157]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mself.hidden_layers = nn.Sequential(*[nn.Linear(input_size, connection_size) for connection_size\u001b[39m\n                                        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m '[' was never closed\n"
     ]
    }
   ],
   "source": [
    "class NodeLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NodeLayer, self).__init__(\n",
    "            nn.Linear(input_size, output_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, num_hidden_layers, output_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        hidden_dims = \n",
    "        \n",
    "        self.hidden_layers = nn.Sequential(*[NodeLayer(input_size, connection_size) for hidden])\n",
    "        self.out_layer = nn.Linear(hidden_dims[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layers(x)\n",
    "        return self.out_layer(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, num_hidden_layers, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        hidden_dims = \n",
    "        \n",
    "        self.hidden_layers = nn.Sequential(*[NodeLayer(input_size, connection_size) for connection_size])\n",
    "        self.out_layer = nn.Linear(hidden_dims[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layers(x)\n",
    "        return self.out_layer(x)\n",
    "\n",
    "class PBMCAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PBMCAutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gk1sfdNe76Kl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjQr4OYW76bN"
   },
   "source": [
    "Train your autoencoding using MSE loss.\n",
    "\n",
    "Finally, identify the parameters which don't overfit, and use the same model architecture and train on all of the data together.\n",
    "\n",
    "With a latent space size of 32, aim for 0.9 MSE loss on your test set, 0.95 with regularization. You will not be graded strictly on a loss cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "u4Q6KU3c8u-E"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "7Trjfxkk8wyg"
   },
   "outputs": [],
   "source": [
    "def train(model, train_data:Dataset, test_data:Dataset, regularization:bool):\n",
    "    # torch.cuda.empty_cache()\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=MODEL_CONFIG['hyperparameters']['BATCH_SIZE'], shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=MODEL_CONFIG['hyperparameters']['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "    # best_val_loss = float('inf')\n",
    "    # best_model = None\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "\n",
    "    mse = nn.MSELoss()\n",
    "    if regularization == True:\n",
    "        l1 = nn.L1Loss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=MODEL_CONFIG['hyperparameters']['LR'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                           factor=MODEL_CONFIG['hyperparameters']['LR_REDUCE_RATE'], \n",
    "                                                           patience=MODEL_CONFIG['hyperparameters']['PATIENCE'])\n",
    "\n",
    "    epochs_pbar = trange(MODEL_CONFIG['hyperparameters']['EPOCHS'], desc='Epochs')\n",
    "    for epoch in epochs_pbar:\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        train_preds, train_targets = [], []\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(data)\n",
    "            if regularization == True:\n",
    "                loss = mse(logits, labels) + l1(logits, labels)\n",
    "            else:\n",
    "                loss = mse(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss.item() * data.size(0)\n",
    "\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            train_preds.extend(predicted.cpu().numpy())\n",
    "            train_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_train_loss /= len(training_data)\n",
    "        epoch_train_acc = accuracy_score(train_targets, train_preds)\n",
    "\n",
    "        # Testing\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        val_preds, val_targets = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in test_loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "                logits = model(imgs)\n",
    "                loss = loss_fn(logits, labels)\n",
    "\n",
    "                epoch_val_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                val_preds.extend(predicted.cpu().numpy())\n",
    "                val_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_val_loss /= len(testing_data)\n",
    "        epoch_val_acc = accuracy_score(val_targets, val_preds)\n",
    "        \n",
    "        print(f'Epoch {epoch+1} -- Val loss: {epoch_val_loss:.4f} Train loss: {epoch_train_loss:.4f}  '\n",
    "              f'Val acc: {epoch_val_acc:.4f} Train acc: {epoch_train_acc:.4f}')\n",
    "\n",
    "        scheduler.step(epoch_val_loss)\n",
    "\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "        history['val_acc'].append(epoch_val_acc)\n",
    "\n",
    "        # if epoch_val_loss > best_val_loss:\n",
    "        #     best_val_loss = epoch_val_loss\n",
    "        #     best_model = deepcopy(model.state_dict())\n",
    "\n",
    "    plot_history(history)\n",
    "            \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'optimizer_state': optimizer.state_dict()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):# Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    model.load_state_dict(best_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1aa70b4532544519bb740d3568551b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -- Val loss: 0.0100 Train loss: 0.0100  Val acc: 0.0100 Test acc: 0.0100\n",
      "Epoch 2 -- Val loss: 0.0100 Train loss: 0.0100  Val acc: 0.0100 Test acc: 0.0100\n",
      "Epoch 3 -- Val loss: 0.0100 Train loss: 0.0100  Val acc: 0.0100 Test acc: 0.0100\n",
      "Epoch 4 -- Val loss: 0.0100 Train loss: 0.0100  Val acc: 0.0100 Test acc: 0.0100\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from random import random\n",
    "epochs_pbar = trange(4, desc='Epochs')\n",
    "for epoch in epochs_pbar:\n",
    "    for j in range(100):\n",
    "        sleep(0.01)\n",
    "    for k in range(100):\n",
    "        sleep(0.01)\n",
    "\n",
    "    print(f'Epoch {epoch+1} -- Val loss: {0.01:.4f} Train loss: {0.01:.4f}  Val acc: {0.01:.4f} Test acc: {0.01:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558641479673423a848aba7b2cbb25a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c69e92e35b4ccb8fd6b84f50298cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccc30eb41ba498cb3c7857018ebc486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ddde6f8560494886b680c4c740815e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2a3882de3d4479bcf3b396b689dfb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e3e09d248b493eb03b1ce92064ffb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b73109c64c043cbbf3de45e61e51cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778402534c334fb4997ea76717b694cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8060582bdf04b31b11d8f0b5f3bdb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with trange(4) as t:\n",
    "    for i in t:\n",
    "        t.set_description(f'Epoch {i+1}')\n",
    "        with trange(100, desc='Train') as tr:\n",
    "            for j in tr:\n",
    "                tr.set_postfix(test_loss=random())\n",
    "                sleep(0.01)\n",
    "        for k in trange(100, desc='Test'):\n",
    "            sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yweQRGit8xDX"
   },
   "source": [
    "Use PCA and t-SNE on the dataset.\n",
    "\n",
    "Then use PCA on the latent space representation of the dataset.\n",
    "\n",
    "Plot all of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGa5B6Ir9KN4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2xcMPP09KxV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfOYsI9S9K5M"
   },
   "source": [
    "Compare the results of PCA, t-SNE, and your autoencoder as ways to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DPmGoHo9uwx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Final Project Part 1 - Autoencoder",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
